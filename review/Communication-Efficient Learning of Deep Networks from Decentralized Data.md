## 出处

Communication-Efficient Learning of Deep Networks
from Decentralized Data

arXiv:1912.04977v1 [cs.LG] 10 Dec 2019

## 目的

本文是联邦学习的开山之作。

介绍联邦学习的背景：用户数据隐私性和数量大，与更新模型的关系

详细成果：实现了基于迭代模型平均化的联邦学习深度网络，同时验证了健壮性，并减少了通信损失

## 背景

- 原来训练模型的方法需要集中获取大量用户数据，从而提供用户体验
- 但是<u>考虑到数据的隐私性和数量大</u>，不能再集中将数据管理到数据中心进行训练
- 提出<u>将训练数据分散在移动设备上</u>，通过<u>整合本地计算的耿勋来学习一个共享模型</u>

## 结论

- 实验表明**联邦学习可以实现，是实用的**。<u>通过与多种模型架构对比</u>*（如一个多层感知器、两个不同的卷积神经网络、一个两层字符LSTM和一个大规模词级LSTM）*，**FedAvg能够使用相对较少的通信轮训练高质量的模型**

## 主要内容

### 引言

- 联邦学习：允许用户分散存储数据，同时用这些数据训练共享模型，并使用该模型。
  - 客户端获取公共模型，利用本地数据训练，得到的训练结果更新提交给服务器即可，不需要上传本地数据
  - 好处：证明了训练模型也可以不用直接接触训练数据；通过限制供给面到用户，从而降低了隐私和安全风险。

- 主要成果（贡献）
  - 具体而言：**引入了FedAvg算法**【客户机局部随机梯度下降(SGD)计算更新，服务器执行模型平均、处理各个更新】
  - 实验表明：该算法<u>对不平衡和非iid数据分布具有鲁棒性</u>，可以在去中心化数据上训练深度网络所需的通信轮数减少数量级（减少通信轮数）。

1. 联邦优化与分布式优化的几个区别

   - **Non-IID 非独立同分布**：每个用户使用产生的数据都不一样，没有代表性
   - **数据不平衡**：每个用户的使用频率不一样，产生的数据是不均等的
   - 大规模分布式：希望参与优化的客户端数目高于平均数量
   - **有限通信**：许多设备不是完全能够进行通讯的，大多是离线或者通讯代价高/效果不佳

   > 也需要中点解决这些问题

2. 联邦优化问题的基本步骤

   - K个客户端，每个都有固定的本地数据集。每轮选择C个客户端，服务器将当前的全局算法状态（当前共享模型）发送给每个客户端

   - 每个选中的客户端基于当前全局模型和本地数据进行本地计算，并将更新发送给服务器。

   - 服务器运用这些更新到全局模型上，实现模型的更新。重复上述过程

   - 优化目标：

     - 所有样本的损失求平均

       <img src="Communication-Efficient%20Learning%20of%20Deep%20Networks%20from%20Decentralized%20Data.assets/image-20210227163851433.png" alt="image-20210227163851433" style="zoom:50%;" />

     - 假设$P_k$是客户端k的数据集，$n_k$是$P_k$的大小。损失函数可以写成如下形式：*（也就是基于每个客户端样本的平均损失，求所有样本的平均损失）*`实际在非iid上不成立`

       <img src="Communication-Efficient%20Learning%20of%20Deep%20Networks%20from%20Decentralized%20Data.assets/image-20210227164244669.png" alt="image-20210227164244669" style="zoom:50%;" />

3. 如何解决FL的通信损失大问题？
   - 分布式学习计算消耗大而通信消耗较小，FL反之
   - 目标：使用额外的计算，以减少训练模型所需的通信轮数。
   - 两种主要增加计算的方法：增加并行程度（每轮选择更多的用户）；增加每个用户的计算量（不再是简单的计算梯度）

### FedAvg

- 用各个用户的平均梯度，得到所有样本（所有用户的）平均梯度；对每个用户先求出对应的参数平均更新，然后把这些更新整合得到所有样本对应的参数更新

  <img src="Communication-Efficient%20Learning%20of%20Deep%20Networks%20from%20Decentralized%20Data.assets/image-20210227171935284.png" alt="image-20210227171935284" style="zoom:50%;" />

- 算法的整体步骤：计算得到每个客户端对参数的更新量，然后**按数量取平均**得到参数最终的更新量，更新参数

  - 参数初始化选择也很重要，会影响模型的损失

<img src="Communication-Efficient%20Learning%20of%20Deep%20Networks%20from%20Decentralized%20Data.assets/image-20210227171734009.png" alt="image-20210227171734009" style="zoom:50%;" />

### 实验结果

> 与三种模型在两类数据集上进行比较

